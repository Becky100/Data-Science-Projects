{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network: Cifar Dataset",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RVUlgfDzOl0n"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "tf.keras.datasets.cifar10.load_data() \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Mfn8BjPQYv",
        "outputId": "73315649-6c95-4059-9397-7c5072dd9531"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([[[[ 59,  62,  63],\n",
              "           [ 43,  46,  45],\n",
              "           [ 50,  48,  43],\n",
              "           ...,\n",
              "           [158, 132, 108],\n",
              "           [152, 125, 102],\n",
              "           [148, 124, 103]],\n",
              "  \n",
              "          [[ 16,  20,  20],\n",
              "           [  0,   0,   0],\n",
              "           [ 18,   8,   0],\n",
              "           ...,\n",
              "           [123,  88,  55],\n",
              "           [119,  83,  50],\n",
              "           [122,  87,  57]],\n",
              "  \n",
              "          [[ 25,  24,  21],\n",
              "           [ 16,   7,   0],\n",
              "           [ 49,  27,   8],\n",
              "           ...,\n",
              "           [118,  84,  50],\n",
              "           [120,  84,  50],\n",
              "           [109,  73,  42]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[208, 170,  96],\n",
              "           [201, 153,  34],\n",
              "           [198, 161,  26],\n",
              "           ...,\n",
              "           [160, 133,  70],\n",
              "           [ 56,  31,   7],\n",
              "           [ 53,  34,  20]],\n",
              "  \n",
              "          [[180, 139,  96],\n",
              "           [173, 123,  42],\n",
              "           [186, 144,  30],\n",
              "           ...,\n",
              "           [184, 148,  94],\n",
              "           [ 97,  62,  34],\n",
              "           [ 83,  53,  34]],\n",
              "  \n",
              "          [[177, 144, 116],\n",
              "           [168, 129,  94],\n",
              "           [179, 142,  87],\n",
              "           ...,\n",
              "           [216, 184, 140],\n",
              "           [151, 118,  84],\n",
              "           [123,  92,  72]]],\n",
              "  \n",
              "  \n",
              "         [[[154, 177, 187],\n",
              "           [126, 137, 136],\n",
              "           [105, 104,  95],\n",
              "           ...,\n",
              "           [ 91,  95,  71],\n",
              "           [ 87,  90,  71],\n",
              "           [ 79,  81,  70]],\n",
              "  \n",
              "          [[140, 160, 169],\n",
              "           [145, 153, 154],\n",
              "           [125, 125, 118],\n",
              "           ...,\n",
              "           [ 96,  99,  78],\n",
              "           [ 77,  80,  62],\n",
              "           [ 71,  73,  61]],\n",
              "  \n",
              "          [[140, 155, 164],\n",
              "           [139, 146, 149],\n",
              "           [115, 115, 112],\n",
              "           ...,\n",
              "           [ 79,  82,  64],\n",
              "           [ 68,  70,  55],\n",
              "           [ 67,  69,  55]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[175, 167, 166],\n",
              "           [156, 154, 160],\n",
              "           [154, 160, 170],\n",
              "           ...,\n",
              "           [ 42,  34,  36],\n",
              "           [ 61,  53,  57],\n",
              "           [ 93,  83,  91]],\n",
              "  \n",
              "          [[165, 154, 128],\n",
              "           [156, 152, 130],\n",
              "           [159, 161, 142],\n",
              "           ...,\n",
              "           [103,  93,  96],\n",
              "           [123, 114, 120],\n",
              "           [131, 121, 131]],\n",
              "  \n",
              "          [[163, 148, 120],\n",
              "           [158, 148, 122],\n",
              "           [163, 156, 133],\n",
              "           ...,\n",
              "           [143, 133, 139],\n",
              "           [143, 134, 142],\n",
              "           [143, 133, 144]]],\n",
              "  \n",
              "  \n",
              "         [[[255, 255, 255],\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253],\n",
              "           ...,\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253],\n",
              "           [253, 253, 253]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           ...,\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255],\n",
              "           [255, 255, 255]],\n",
              "  \n",
              "          [[255, 255, 255],\n",
              "           [254, 254, 254],\n",
              "           [254, 254, 254],\n",
              "           ...,\n",
              "           [254, 254, 254],\n",
              "           [254, 254, 254],\n",
              "           [254, 254, 254]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[113, 120, 112],\n",
              "           [111, 118, 111],\n",
              "           [105, 112, 106],\n",
              "           ...,\n",
              "           [ 72,  81,  80],\n",
              "           [ 72,  80,  79],\n",
              "           [ 72,  80,  79]],\n",
              "  \n",
              "          [[111, 118, 110],\n",
              "           [104, 111, 104],\n",
              "           [ 99, 106,  98],\n",
              "           ...,\n",
              "           [ 68,  75,  73],\n",
              "           [ 70,  76,  75],\n",
              "           [ 78,  84,  82]],\n",
              "  \n",
              "          [[106, 113, 105],\n",
              "           [ 99, 106,  98],\n",
              "           [ 95, 102,  94],\n",
              "           ...,\n",
              "           [ 78,  85,  83],\n",
              "           [ 79,  85,  83],\n",
              "           [ 80,  86,  84]]],\n",
              "  \n",
              "  \n",
              "         ...,\n",
              "  \n",
              "  \n",
              "         [[[ 35, 178, 235],\n",
              "           [ 40, 176, 239],\n",
              "           [ 42, 176, 241],\n",
              "           ...,\n",
              "           [ 99, 177, 219],\n",
              "           [ 79, 147, 197],\n",
              "           [ 89, 148, 189]],\n",
              "  \n",
              "          [[ 57, 182, 234],\n",
              "           [ 44, 184, 250],\n",
              "           [ 50, 183, 240],\n",
              "           ...,\n",
              "           [156, 182, 200],\n",
              "           [141, 177, 206],\n",
              "           [116, 149, 175]],\n",
              "  \n",
              "          [[ 98, 197, 237],\n",
              "           [ 64, 189, 252],\n",
              "           [ 69, 192, 245],\n",
              "           ...,\n",
              "           [188, 195, 206],\n",
              "           [119, 135, 147],\n",
              "           [ 61,  79,  90]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 73,  79,  77],\n",
              "           [ 53,  63,  68],\n",
              "           [ 54,  68,  80],\n",
              "           ...,\n",
              "           [ 17,  40,  64],\n",
              "           [ 21,  36,  51],\n",
              "           [ 33,  48,  49]],\n",
              "  \n",
              "          [[ 61,  68,  75],\n",
              "           [ 55,  70,  86],\n",
              "           [ 57,  79, 103],\n",
              "           ...,\n",
              "           [ 24,  48,  72],\n",
              "           [ 17,  35,  53],\n",
              "           [  7,  23,  32]],\n",
              "  \n",
              "          [[ 44,  56,  73],\n",
              "           [ 46,  66,  88],\n",
              "           [ 49,  77, 105],\n",
              "           ...,\n",
              "           [ 27,  52,  77],\n",
              "           [ 21,  43,  66],\n",
              "           [ 12,  31,  50]]],\n",
              "  \n",
              "  \n",
              "         [[[189, 211, 240],\n",
              "           [186, 208, 236],\n",
              "           [185, 207, 235],\n",
              "           ...,\n",
              "           [175, 195, 224],\n",
              "           [172, 194, 222],\n",
              "           [169, 194, 220]],\n",
              "  \n",
              "          [[194, 210, 239],\n",
              "           [191, 207, 236],\n",
              "           [190, 206, 235],\n",
              "           ...,\n",
              "           [173, 192, 220],\n",
              "           [171, 191, 218],\n",
              "           [167, 190, 216]],\n",
              "  \n",
              "          [[208, 219, 244],\n",
              "           [205, 216, 240],\n",
              "           [204, 215, 239],\n",
              "           ...,\n",
              "           [175, 191, 217],\n",
              "           [172, 190, 216],\n",
              "           [169, 191, 215]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[207, 199, 181],\n",
              "           [203, 195, 175],\n",
              "           [203, 196, 173],\n",
              "           ...,\n",
              "           [135, 132, 127],\n",
              "           [162, 158, 150],\n",
              "           [168, 163, 151]],\n",
              "  \n",
              "          [[198, 190, 170],\n",
              "           [189, 181, 159],\n",
              "           [180, 172, 147],\n",
              "           ...,\n",
              "           [178, 171, 160],\n",
              "           [175, 169, 156],\n",
              "           [175, 169, 154]],\n",
              "  \n",
              "          [[198, 189, 173],\n",
              "           [189, 181, 162],\n",
              "           [178, 170, 149],\n",
              "           ...,\n",
              "           [195, 184, 169],\n",
              "           [196, 189, 171],\n",
              "           [195, 190, 171]]],\n",
              "  \n",
              "  \n",
              "         [[[229, 229, 239],\n",
              "           [236, 237, 247],\n",
              "           [234, 236, 247],\n",
              "           ...,\n",
              "           [217, 219, 233],\n",
              "           [221, 223, 234],\n",
              "           [222, 223, 233]],\n",
              "  \n",
              "          [[222, 221, 229],\n",
              "           [239, 239, 249],\n",
              "           [233, 234, 246],\n",
              "           ...,\n",
              "           [223, 223, 236],\n",
              "           [227, 228, 238],\n",
              "           [210, 211, 220]],\n",
              "  \n",
              "          [[213, 206, 211],\n",
              "           [234, 232, 239],\n",
              "           [231, 233, 244],\n",
              "           ...,\n",
              "           [220, 220, 232],\n",
              "           [220, 219, 232],\n",
              "           [202, 203, 215]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[150, 143, 135],\n",
              "           [140, 135, 127],\n",
              "           [132, 127, 120],\n",
              "           ...,\n",
              "           [224, 222, 218],\n",
              "           [230, 228, 225],\n",
              "           [241, 241, 238]],\n",
              "  \n",
              "          [[137, 132, 126],\n",
              "           [130, 127, 120],\n",
              "           [125, 121, 115],\n",
              "           ...,\n",
              "           [181, 180, 178],\n",
              "           [202, 201, 198],\n",
              "           [212, 211, 207]],\n",
              "  \n",
              "          [[122, 119, 114],\n",
              "           [118, 116, 110],\n",
              "           [120, 116, 111],\n",
              "           ...,\n",
              "           [179, 177, 173],\n",
              "           [164, 164, 162],\n",
              "           [163, 163, 161]]]], dtype=uint8), array([[6],\n",
              "         [9],\n",
              "         [9],\n",
              "         ...,\n",
              "         [9],\n",
              "         [1],\n",
              "         [1]], dtype=uint8)), (array([[[[158, 112,  49],\n",
              "           [159, 111,  47],\n",
              "           [165, 116,  51],\n",
              "           ...,\n",
              "           [137,  95,  36],\n",
              "           [126,  91,  36],\n",
              "           [116,  85,  33]],\n",
              "  \n",
              "          [[152, 112,  51],\n",
              "           [151, 110,  40],\n",
              "           [159, 114,  45],\n",
              "           ...,\n",
              "           [136,  95,  31],\n",
              "           [125,  91,  32],\n",
              "           [119,  88,  34]],\n",
              "  \n",
              "          [[151, 110,  47],\n",
              "           [151, 109,  33],\n",
              "           [158, 111,  36],\n",
              "           ...,\n",
              "           [139,  98,  34],\n",
              "           [130,  95,  34],\n",
              "           [120,  89,  33]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 68, 124, 177],\n",
              "           [ 42, 100, 148],\n",
              "           [ 31,  88, 137],\n",
              "           ...,\n",
              "           [ 38,  97, 146],\n",
              "           [ 13,  64, 108],\n",
              "           [ 40,  85, 127]],\n",
              "  \n",
              "          [[ 61, 116, 168],\n",
              "           [ 49, 102, 148],\n",
              "           [ 35,  85, 132],\n",
              "           ...,\n",
              "           [ 26,  82, 130],\n",
              "           [ 29,  82, 126],\n",
              "           [ 20,  64, 107]],\n",
              "  \n",
              "          [[ 54, 107, 160],\n",
              "           [ 56, 105, 149],\n",
              "           [ 45,  89, 132],\n",
              "           ...,\n",
              "           [ 24,  77, 124],\n",
              "           [ 34,  84, 129],\n",
              "           [ 21,  67, 110]]],\n",
              "  \n",
              "  \n",
              "         [[[235, 235, 235],\n",
              "           [231, 231, 231],\n",
              "           [232, 232, 232],\n",
              "           ...,\n",
              "           [233, 233, 233],\n",
              "           [233, 233, 233],\n",
              "           [232, 232, 232]],\n",
              "  \n",
              "          [[238, 238, 238],\n",
              "           [235, 235, 235],\n",
              "           [235, 235, 235],\n",
              "           ...,\n",
              "           [236, 236, 236],\n",
              "           [236, 236, 236],\n",
              "           [235, 235, 235]],\n",
              "  \n",
              "          [[237, 237, 237],\n",
              "           [234, 234, 234],\n",
              "           [234, 234, 234],\n",
              "           ...,\n",
              "           [235, 235, 235],\n",
              "           [235, 235, 235],\n",
              "           [234, 234, 234]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 87,  99,  89],\n",
              "           [ 43,  51,  37],\n",
              "           [ 19,  23,  11],\n",
              "           ...,\n",
              "           [169, 184, 179],\n",
              "           [182, 197, 193],\n",
              "           [188, 202, 201]],\n",
              "  \n",
              "          [[ 82,  96,  82],\n",
              "           [ 46,  57,  36],\n",
              "           [ 36,  44,  22],\n",
              "           ...,\n",
              "           [174, 189, 183],\n",
              "           [185, 200, 196],\n",
              "           [187, 202, 200]],\n",
              "  \n",
              "          [[ 85, 101,  83],\n",
              "           [ 62,  75,  48],\n",
              "           [ 58,  67,  38],\n",
              "           ...,\n",
              "           [168, 183, 178],\n",
              "           [180, 195, 191],\n",
              "           [186, 200, 199]]],\n",
              "  \n",
              "  \n",
              "         [[[158, 190, 222],\n",
              "           [158, 187, 218],\n",
              "           [139, 166, 194],\n",
              "           ...,\n",
              "           [228, 231, 234],\n",
              "           [237, 239, 243],\n",
              "           [238, 241, 246]],\n",
              "  \n",
              "          [[170, 200, 229],\n",
              "           [172, 199, 226],\n",
              "           [151, 176, 201],\n",
              "           ...,\n",
              "           [232, 232, 236],\n",
              "           [246, 246, 250],\n",
              "           [246, 247, 251]],\n",
              "  \n",
              "          [[174, 201, 225],\n",
              "           [176, 200, 222],\n",
              "           [157, 179, 199],\n",
              "           ...,\n",
              "           [230, 229, 232],\n",
              "           [250, 249, 251],\n",
              "           [245, 244, 247]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 31,  40,  45],\n",
              "           [ 30,  39,  44],\n",
              "           [ 26,  35,  40],\n",
              "           ...,\n",
              "           [ 37,  40,  46],\n",
              "           [  9,  13,  14],\n",
              "           [  4,   7,   5]],\n",
              "  \n",
              "          [[ 23,  34,  39],\n",
              "           [ 27,  38,  43],\n",
              "           [ 25,  36,  41],\n",
              "           ...,\n",
              "           [ 19,  20,  24],\n",
              "           [  4,   6,   3],\n",
              "           [  5,   7,   3]],\n",
              "  \n",
              "          [[ 28,  41,  47],\n",
              "           [ 30,  43,  50],\n",
              "           [ 32,  45,  52],\n",
              "           ...,\n",
              "           [  5,   6,   8],\n",
              "           [  4,   5,   3],\n",
              "           [  7,   8,   7]]],\n",
              "  \n",
              "  \n",
              "         ...,\n",
              "  \n",
              "  \n",
              "         [[[ 20,  15,  12],\n",
              "           [ 19,  14,  11],\n",
              "           [ 15,  14,  11],\n",
              "           ...,\n",
              "           [ 10,   9,   7],\n",
              "           [ 12,  11,   9],\n",
              "           [ 13,  12,  10]],\n",
              "  \n",
              "          [[ 21,  16,  13],\n",
              "           [ 20,  16,  13],\n",
              "           [ 18,  17,  12],\n",
              "           ...,\n",
              "           [ 10,   9,   7],\n",
              "           [ 10,   9,   7],\n",
              "           [ 12,  11,   9]],\n",
              "  \n",
              "          [[ 21,  16,  13],\n",
              "           [ 21,  17,  12],\n",
              "           [ 20,  18,  11],\n",
              "           ...,\n",
              "           [ 12,  11,   9],\n",
              "           [ 12,  11,   9],\n",
              "           [ 13,  12,  10]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[ 33,  25,  13],\n",
              "           [ 34,  26,  15],\n",
              "           [ 34,  26,  15],\n",
              "           ...,\n",
              "           [ 28,  25,  52],\n",
              "           [ 29,  25,  58],\n",
              "           [ 23,  20,  42]],\n",
              "  \n",
              "          [[ 33,  25,  14],\n",
              "           [ 34,  26,  15],\n",
              "           [ 34,  26,  15],\n",
              "           ...,\n",
              "           [ 27,  24,  52],\n",
              "           [ 27,  24,  56],\n",
              "           [ 25,  22,  47]],\n",
              "  \n",
              "          [[ 31,  23,  12],\n",
              "           [ 32,  24,  13],\n",
              "           [ 33,  25,  14],\n",
              "           ...,\n",
              "           [ 24,  23,  50],\n",
              "           [ 26,  23,  53],\n",
              "           [ 25,  20,  47]]],\n",
              "  \n",
              "  \n",
              "         [[[ 25,  40,  12],\n",
              "           [ 15,  36,   3],\n",
              "           [ 23,  41,  18],\n",
              "           ...,\n",
              "           [ 61,  82,  78],\n",
              "           [ 92, 113, 112],\n",
              "           [ 75,  89,  92]],\n",
              "  \n",
              "          [[ 12,  25,   6],\n",
              "           [ 20,  37,   7],\n",
              "           [ 24,  36,  15],\n",
              "           ...,\n",
              "           [115, 134, 138],\n",
              "           [149, 168, 177],\n",
              "           [104, 117, 131]],\n",
              "  \n",
              "          [[ 12,  25,  11],\n",
              "           [ 15,  29,   6],\n",
              "           [ 34,  40,  24],\n",
              "           ...,\n",
              "           [154, 172, 182],\n",
              "           [157, 175, 192],\n",
              "           [116, 129, 151]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[100, 129,  81],\n",
              "           [103, 132,  84],\n",
              "           [104, 134,  86],\n",
              "           ...,\n",
              "           [ 97, 128,  84],\n",
              "           [ 98, 126,  84],\n",
              "           [ 91, 121,  79]],\n",
              "  \n",
              "          [[103, 132,  83],\n",
              "           [104, 131,  83],\n",
              "           [107, 135,  87],\n",
              "           ...,\n",
              "           [101, 132,  87],\n",
              "           [ 99, 127,  84],\n",
              "           [ 92, 121,  79]],\n",
              "  \n",
              "          [[ 95, 126,  78],\n",
              "           [ 95, 123,  76],\n",
              "           [101, 128,  81],\n",
              "           ...,\n",
              "           [ 93, 124,  80],\n",
              "           [ 95, 123,  81],\n",
              "           [ 92, 120,  80]]],\n",
              "  \n",
              "  \n",
              "         [[[ 73,  78,  75],\n",
              "           [ 98, 103, 113],\n",
              "           [ 99, 106, 114],\n",
              "           ...,\n",
              "           [135, 150, 152],\n",
              "           [135, 149, 154],\n",
              "           [203, 215, 223]],\n",
              "  \n",
              "          [[ 69,  73,  70],\n",
              "           [ 84,  89,  97],\n",
              "           [ 68,  75,  81],\n",
              "           ...,\n",
              "           [ 85,  95,  89],\n",
              "           [ 71,  82,  80],\n",
              "           [120, 133, 135]],\n",
              "  \n",
              "          [[ 69,  73,  70],\n",
              "           [ 90,  95, 100],\n",
              "           [ 62,  71,  74],\n",
              "           ...,\n",
              "           [ 74,  81,  70],\n",
              "           [ 53,  62,  54],\n",
              "           [ 62,  74,  69]],\n",
              "  \n",
              "          ...,\n",
              "  \n",
              "          [[123, 128,  96],\n",
              "           [132, 132, 102],\n",
              "           [129, 128, 100],\n",
              "           ...,\n",
              "           [108, 107,  88],\n",
              "           [ 62,  60,  55],\n",
              "           [ 27,  27,  28]],\n",
              "  \n",
              "          [[115, 121,  91],\n",
              "           [123, 124,  95],\n",
              "           [129, 126,  99],\n",
              "           ...,\n",
              "           [115, 116,  94],\n",
              "           [ 66,  65,  59],\n",
              "           [ 27,  27,  27]],\n",
              "  \n",
              "          [[116, 120,  90],\n",
              "           [121, 122,  94],\n",
              "           [129, 128, 101],\n",
              "           ...,\n",
              "           [116, 115,  94],\n",
              "           [ 68,  65,  58],\n",
              "           [ 27,  26,  26]]]], dtype=uint8), array([[3],\n",
              "         [8],\n",
              "         [8],\n",
              "         ...,\n",
              "         [5],\n",
              "         [1],\n",
              "         [7]], dtype=uint8)))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "KK7Wk6EQPXzq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the shape of train and test images. CNN requires 3D images.\n",
        "#To save on memory you can reshape to a gray scale image by substituting 3 and 1\n",
        "x=train_images.shape\n",
        "y=test_images.shape\n",
        "x,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqGRntz3PcqT",
        "outputId": "6b76c872-425d-43e2-8576-a66499b4c7a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize the data to make sure that all dimensions are on the same scale\n",
        "train_images_n=train_images/255.\n",
        "test_images_n=test_images/255.\n",
        "train_images_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb5aRtIzPdkJ",
        "outputId": "845b9674-05a7-4dbf-a7b8-7ffa6c8f0b3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.23137255, 0.24313725, 0.24705882],\n",
              "         [0.16862745, 0.18039216, 0.17647059],\n",
              "         [0.19607843, 0.18823529, 0.16862745],\n",
              "         ...,\n",
              "         [0.61960784, 0.51764706, 0.42352941],\n",
              "         [0.59607843, 0.49019608, 0.4       ],\n",
              "         [0.58039216, 0.48627451, 0.40392157]],\n",
              "\n",
              "        [[0.0627451 , 0.07843137, 0.07843137],\n",
              "         [0.        , 0.        , 0.        ],\n",
              "         [0.07058824, 0.03137255, 0.        ],\n",
              "         ...,\n",
              "         [0.48235294, 0.34509804, 0.21568627],\n",
              "         [0.46666667, 0.3254902 , 0.19607843],\n",
              "         [0.47843137, 0.34117647, 0.22352941]],\n",
              "\n",
              "        [[0.09803922, 0.09411765, 0.08235294],\n",
              "         [0.0627451 , 0.02745098, 0.        ],\n",
              "         [0.19215686, 0.10588235, 0.03137255],\n",
              "         ...,\n",
              "         [0.4627451 , 0.32941176, 0.19607843],\n",
              "         [0.47058824, 0.32941176, 0.19607843],\n",
              "         [0.42745098, 0.28627451, 0.16470588]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.81568627, 0.66666667, 0.37647059],\n",
              "         [0.78823529, 0.6       , 0.13333333],\n",
              "         [0.77647059, 0.63137255, 0.10196078],\n",
              "         ...,\n",
              "         [0.62745098, 0.52156863, 0.2745098 ],\n",
              "         [0.21960784, 0.12156863, 0.02745098],\n",
              "         [0.20784314, 0.13333333, 0.07843137]],\n",
              "\n",
              "        [[0.70588235, 0.54509804, 0.37647059],\n",
              "         [0.67843137, 0.48235294, 0.16470588],\n",
              "         [0.72941176, 0.56470588, 0.11764706],\n",
              "         ...,\n",
              "         [0.72156863, 0.58039216, 0.36862745],\n",
              "         [0.38039216, 0.24313725, 0.13333333],\n",
              "         [0.3254902 , 0.20784314, 0.13333333]],\n",
              "\n",
              "        [[0.69411765, 0.56470588, 0.45490196],\n",
              "         [0.65882353, 0.50588235, 0.36862745],\n",
              "         [0.70196078, 0.55686275, 0.34117647],\n",
              "         ...,\n",
              "         [0.84705882, 0.72156863, 0.54901961],\n",
              "         [0.59215686, 0.4627451 , 0.32941176],\n",
              "         [0.48235294, 0.36078431, 0.28235294]]],\n",
              "\n",
              "\n",
              "       [[[0.60392157, 0.69411765, 0.73333333],\n",
              "         [0.49411765, 0.5372549 , 0.53333333],\n",
              "         [0.41176471, 0.40784314, 0.37254902],\n",
              "         ...,\n",
              "         [0.35686275, 0.37254902, 0.27843137],\n",
              "         [0.34117647, 0.35294118, 0.27843137],\n",
              "         [0.30980392, 0.31764706, 0.2745098 ]],\n",
              "\n",
              "        [[0.54901961, 0.62745098, 0.6627451 ],\n",
              "         [0.56862745, 0.6       , 0.60392157],\n",
              "         [0.49019608, 0.49019608, 0.4627451 ],\n",
              "         ...,\n",
              "         [0.37647059, 0.38823529, 0.30588235],\n",
              "         [0.30196078, 0.31372549, 0.24313725],\n",
              "         [0.27843137, 0.28627451, 0.23921569]],\n",
              "\n",
              "        [[0.54901961, 0.60784314, 0.64313725],\n",
              "         [0.54509804, 0.57254902, 0.58431373],\n",
              "         [0.45098039, 0.45098039, 0.43921569],\n",
              "         ...,\n",
              "         [0.30980392, 0.32156863, 0.25098039],\n",
              "         [0.26666667, 0.2745098 , 0.21568627],\n",
              "         [0.2627451 , 0.27058824, 0.21568627]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.68627451, 0.65490196, 0.65098039],\n",
              "         [0.61176471, 0.60392157, 0.62745098],\n",
              "         [0.60392157, 0.62745098, 0.66666667],\n",
              "         ...,\n",
              "         [0.16470588, 0.13333333, 0.14117647],\n",
              "         [0.23921569, 0.20784314, 0.22352941],\n",
              "         [0.36470588, 0.3254902 , 0.35686275]],\n",
              "\n",
              "        [[0.64705882, 0.60392157, 0.50196078],\n",
              "         [0.61176471, 0.59607843, 0.50980392],\n",
              "         [0.62352941, 0.63137255, 0.55686275],\n",
              "         ...,\n",
              "         [0.40392157, 0.36470588, 0.37647059],\n",
              "         [0.48235294, 0.44705882, 0.47058824],\n",
              "         [0.51372549, 0.4745098 , 0.51372549]],\n",
              "\n",
              "        [[0.63921569, 0.58039216, 0.47058824],\n",
              "         [0.61960784, 0.58039216, 0.47843137],\n",
              "         [0.63921569, 0.61176471, 0.52156863],\n",
              "         ...,\n",
              "         [0.56078431, 0.52156863, 0.54509804],\n",
              "         [0.56078431, 0.5254902 , 0.55686275],\n",
              "         [0.56078431, 0.52156863, 0.56470588]]],\n",
              "\n",
              "\n",
              "       [[[1.        , 1.        , 1.        ],\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         ...,\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         [0.99215686, 0.99215686, 0.99215686],\n",
              "         [0.99215686, 0.99215686, 0.99215686]],\n",
              "\n",
              "        [[1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         ...,\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ],\n",
              "         [1.        , 1.        , 1.        ]],\n",
              "\n",
              "        [[1.        , 1.        , 1.        ],\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         ...,\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         [0.99607843, 0.99607843, 0.99607843],\n",
              "         [0.99607843, 0.99607843, 0.99607843]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.44313725, 0.47058824, 0.43921569],\n",
              "         [0.43529412, 0.4627451 , 0.43529412],\n",
              "         [0.41176471, 0.43921569, 0.41568627],\n",
              "         ...,\n",
              "         [0.28235294, 0.31764706, 0.31372549],\n",
              "         [0.28235294, 0.31372549, 0.30980392],\n",
              "         [0.28235294, 0.31372549, 0.30980392]],\n",
              "\n",
              "        [[0.43529412, 0.4627451 , 0.43137255],\n",
              "         [0.40784314, 0.43529412, 0.40784314],\n",
              "         [0.38823529, 0.41568627, 0.38431373],\n",
              "         ...,\n",
              "         [0.26666667, 0.29411765, 0.28627451],\n",
              "         [0.2745098 , 0.29803922, 0.29411765],\n",
              "         [0.30588235, 0.32941176, 0.32156863]],\n",
              "\n",
              "        [[0.41568627, 0.44313725, 0.41176471],\n",
              "         [0.38823529, 0.41568627, 0.38431373],\n",
              "         [0.37254902, 0.4       , 0.36862745],\n",
              "         ...,\n",
              "         [0.30588235, 0.33333333, 0.3254902 ],\n",
              "         [0.30980392, 0.33333333, 0.3254902 ],\n",
              "         [0.31372549, 0.3372549 , 0.32941176]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0.1372549 , 0.69803922, 0.92156863],\n",
              "         [0.15686275, 0.69019608, 0.9372549 ],\n",
              "         [0.16470588, 0.69019608, 0.94509804],\n",
              "         ...,\n",
              "         [0.38823529, 0.69411765, 0.85882353],\n",
              "         [0.30980392, 0.57647059, 0.77254902],\n",
              "         [0.34901961, 0.58039216, 0.74117647]],\n",
              "\n",
              "        [[0.22352941, 0.71372549, 0.91764706],\n",
              "         [0.17254902, 0.72156863, 0.98039216],\n",
              "         [0.19607843, 0.71764706, 0.94117647],\n",
              "         ...,\n",
              "         [0.61176471, 0.71372549, 0.78431373],\n",
              "         [0.55294118, 0.69411765, 0.80784314],\n",
              "         [0.45490196, 0.58431373, 0.68627451]],\n",
              "\n",
              "        [[0.38431373, 0.77254902, 0.92941176],\n",
              "         [0.25098039, 0.74117647, 0.98823529],\n",
              "         [0.27058824, 0.75294118, 0.96078431],\n",
              "         ...,\n",
              "         [0.7372549 , 0.76470588, 0.80784314],\n",
              "         [0.46666667, 0.52941176, 0.57647059],\n",
              "         [0.23921569, 0.30980392, 0.35294118]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.28627451, 0.30980392, 0.30196078],\n",
              "         [0.20784314, 0.24705882, 0.26666667],\n",
              "         [0.21176471, 0.26666667, 0.31372549],\n",
              "         ...,\n",
              "         [0.06666667, 0.15686275, 0.25098039],\n",
              "         [0.08235294, 0.14117647, 0.2       ],\n",
              "         [0.12941176, 0.18823529, 0.19215686]],\n",
              "\n",
              "        [[0.23921569, 0.26666667, 0.29411765],\n",
              "         [0.21568627, 0.2745098 , 0.3372549 ],\n",
              "         [0.22352941, 0.30980392, 0.40392157],\n",
              "         ...,\n",
              "         [0.09411765, 0.18823529, 0.28235294],\n",
              "         [0.06666667, 0.1372549 , 0.20784314],\n",
              "         [0.02745098, 0.09019608, 0.1254902 ]],\n",
              "\n",
              "        [[0.17254902, 0.21960784, 0.28627451],\n",
              "         [0.18039216, 0.25882353, 0.34509804],\n",
              "         [0.19215686, 0.30196078, 0.41176471],\n",
              "         ...,\n",
              "         [0.10588235, 0.20392157, 0.30196078],\n",
              "         [0.08235294, 0.16862745, 0.25882353],\n",
              "         [0.04705882, 0.12156863, 0.19607843]]],\n",
              "\n",
              "\n",
              "       [[[0.74117647, 0.82745098, 0.94117647],\n",
              "         [0.72941176, 0.81568627, 0.9254902 ],\n",
              "         [0.7254902 , 0.81176471, 0.92156863],\n",
              "         ...,\n",
              "         [0.68627451, 0.76470588, 0.87843137],\n",
              "         [0.6745098 , 0.76078431, 0.87058824],\n",
              "         [0.6627451 , 0.76078431, 0.8627451 ]],\n",
              "\n",
              "        [[0.76078431, 0.82352941, 0.9372549 ],\n",
              "         [0.74901961, 0.81176471, 0.9254902 ],\n",
              "         [0.74509804, 0.80784314, 0.92156863],\n",
              "         ...,\n",
              "         [0.67843137, 0.75294118, 0.8627451 ],\n",
              "         [0.67058824, 0.74901961, 0.85490196],\n",
              "         [0.65490196, 0.74509804, 0.84705882]],\n",
              "\n",
              "        [[0.81568627, 0.85882353, 0.95686275],\n",
              "         [0.80392157, 0.84705882, 0.94117647],\n",
              "         [0.8       , 0.84313725, 0.9372549 ],\n",
              "         ...,\n",
              "         [0.68627451, 0.74901961, 0.85098039],\n",
              "         [0.6745098 , 0.74509804, 0.84705882],\n",
              "         [0.6627451 , 0.74901961, 0.84313725]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.81176471, 0.78039216, 0.70980392],\n",
              "         [0.79607843, 0.76470588, 0.68627451],\n",
              "         [0.79607843, 0.76862745, 0.67843137],\n",
              "         ...,\n",
              "         [0.52941176, 0.51764706, 0.49803922],\n",
              "         [0.63529412, 0.61960784, 0.58823529],\n",
              "         [0.65882353, 0.63921569, 0.59215686]],\n",
              "\n",
              "        [[0.77647059, 0.74509804, 0.66666667],\n",
              "         [0.74117647, 0.70980392, 0.62352941],\n",
              "         [0.70588235, 0.6745098 , 0.57647059],\n",
              "         ...,\n",
              "         [0.69803922, 0.67058824, 0.62745098],\n",
              "         [0.68627451, 0.6627451 , 0.61176471],\n",
              "         [0.68627451, 0.6627451 , 0.60392157]],\n",
              "\n",
              "        [[0.77647059, 0.74117647, 0.67843137],\n",
              "         [0.74117647, 0.70980392, 0.63529412],\n",
              "         [0.69803922, 0.66666667, 0.58431373],\n",
              "         ...,\n",
              "         [0.76470588, 0.72156863, 0.6627451 ],\n",
              "         [0.76862745, 0.74117647, 0.67058824],\n",
              "         [0.76470588, 0.74509804, 0.67058824]]],\n",
              "\n",
              "\n",
              "       [[[0.89803922, 0.89803922, 0.9372549 ],\n",
              "         [0.9254902 , 0.92941176, 0.96862745],\n",
              "         [0.91764706, 0.9254902 , 0.96862745],\n",
              "         ...,\n",
              "         [0.85098039, 0.85882353, 0.91372549],\n",
              "         [0.86666667, 0.8745098 , 0.91764706],\n",
              "         [0.87058824, 0.8745098 , 0.91372549]],\n",
              "\n",
              "        [[0.87058824, 0.86666667, 0.89803922],\n",
              "         [0.9372549 , 0.9372549 , 0.97647059],\n",
              "         [0.91372549, 0.91764706, 0.96470588],\n",
              "         ...,\n",
              "         [0.8745098 , 0.8745098 , 0.9254902 ],\n",
              "         [0.89019608, 0.89411765, 0.93333333],\n",
              "         [0.82352941, 0.82745098, 0.8627451 ]],\n",
              "\n",
              "        [[0.83529412, 0.80784314, 0.82745098],\n",
              "         [0.91764706, 0.90980392, 0.9372549 ],\n",
              "         [0.90588235, 0.91372549, 0.95686275],\n",
              "         ...,\n",
              "         [0.8627451 , 0.8627451 , 0.90980392],\n",
              "         [0.8627451 , 0.85882353, 0.90980392],\n",
              "         [0.79215686, 0.79607843, 0.84313725]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.58823529, 0.56078431, 0.52941176],\n",
              "         [0.54901961, 0.52941176, 0.49803922],\n",
              "         [0.51764706, 0.49803922, 0.47058824],\n",
              "         ...,\n",
              "         [0.87843137, 0.87058824, 0.85490196],\n",
              "         [0.90196078, 0.89411765, 0.88235294],\n",
              "         [0.94509804, 0.94509804, 0.93333333]],\n",
              "\n",
              "        [[0.5372549 , 0.51764706, 0.49411765],\n",
              "         [0.50980392, 0.49803922, 0.47058824],\n",
              "         [0.49019608, 0.4745098 , 0.45098039],\n",
              "         ...,\n",
              "         [0.70980392, 0.70588235, 0.69803922],\n",
              "         [0.79215686, 0.78823529, 0.77647059],\n",
              "         [0.83137255, 0.82745098, 0.81176471]],\n",
              "\n",
              "        [[0.47843137, 0.46666667, 0.44705882],\n",
              "         [0.4627451 , 0.45490196, 0.43137255],\n",
              "         [0.47058824, 0.45490196, 0.43529412],\n",
              "         ...,\n",
              "         [0.70196078, 0.69411765, 0.67843137],\n",
              "         [0.64313725, 0.64313725, 0.63529412],\n",
              "         [0.63921569, 0.63921569, 0.63137255]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we have train and test data. split the train data further to train/validation datasets\n",
        "val_images,train_images=train_images_n[:5000],train_images_n[5000:]\n",
        "val_labels,train_labels=train_labels[:5000],train_labels[5000:]\n",
        "test_images=test_images_n"
      ],
      "metadata": {
        "id": "to9Y1rsoPhI7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W16KEA5MPmAb",
        "outputId": "7df8bc17-4a73-46c8-8286-e18ecff0a316"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.44.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.2 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "i545P37RPqM1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=train_images.reshape(-1,32,32,3)\n",
        "x_test=test_images.reshape(10000,32,32,3)"
      ],
      "metadata": {
        "id": "XgwgyjR0Puud"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HF6lp4cPxsf",
        "outputId": "2546a1c2-6e60-4711-a6c9-f51156fb38ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):  # random search passes this hyperparameter() object \n",
        "    model = keras.models.Sequential()\n",
        "    \n",
        "    model.add(Conv2D(hp.Int('input_units',\n",
        "                                min_value=32,\n",
        "                                max_value=1000,\n",
        "                                step=32), (3, 3), input_shape=x_train.shape[1:]))\n",
        "    \n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    for i in range(hp.Int('conv_layers', 1, 4)):  # adding variation of layers.\n",
        "        model.add(Conv2D(hp.Int(f'conv_{i}_units',\n",
        "                                min_value=32,\n",
        "                                max_value=1000,\n",
        "                                step=32), (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "    model.add(Flatten()) \n",
        "    def build_model(hp):\n",
        "       for i in range(hp.Int('nn_layers', 1, 4)):  # adding variation of layers.\n",
        "        model.add(Conv2D(hp.Int(f'neurons',\n",
        "                                min_value=32,\n",
        "                                max_value=1000,\n",
        "                                step=32), (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "hhhKRhwbxcNz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the tuner\n",
        "tuner = kt.Hyperband(hypermodel=build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='CNN HYPERBAND TUNING')"
      ],
      "metadata": {
        "id": "UuL1x8EPSSUP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#a callback to stop training early after reaching a certain value for the validation loss\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)"
      ],
      "metadata": {
        "id": "brEzh661TTU7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, train_labels, epochs=10, callbacks=[stop_early],validation_data=(val_images, val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-YEaixqK_sv",
        "outputId": "a4a51a16-3ee6-4718-df41-56203b48bd8e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 31m 07s]\n",
            "val_accuracy: 0.7730000019073486\n",
            "\n",
            "Best val_accuracy So Far: 0.7730000019073486\n",
            "Total elapsed time: 03h 55m 18s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the densely-connected\n",
        "layer is {best_hps.get('nn_layers')}, The optimal number of conv units is {best_hps.get('conv_layers')}  and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "OCJ_uQA6u1PN",
        "outputId": "73864cd0-25a9-448f-e2a8-75ec7036d992"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 0))\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-92d8057bcd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nn_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThe\u001b[0m \u001b[0moptimal\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mconv\u001b[0m \u001b[0munits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moptimal\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \"\"\")\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/hyperparameters.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is currently inactive.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} does not exist.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'nn_layers does not exist.'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBbtRD6ZvzvU",
        "outputId": "fd462b16-b23c-4433-ee85-920c6a2fb74a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1407/1407 [==============================] - 184s 130ms/step - loss: 1.5040 - accuracy: 0.4544 - val_loss: 1.2361 - val_accuracy: 0.5724\n",
            "Epoch 2/10\n",
            "1407/1407 [==============================] - 184s 131ms/step - loss: 1.0819 - accuracy: 0.6213 - val_loss: 0.9538 - val_accuracy: 0.6730\n",
            "Epoch 3/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.8722 - accuracy: 0.6968 - val_loss: 0.8378 - val_accuracy: 0.7102\n",
            "Epoch 4/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.7260 - accuracy: 0.7467 - val_loss: 0.7984 - val_accuracy: 0.7178\n",
            "Epoch 5/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.6151 - accuracy: 0.7856 - val_loss: 0.7232 - val_accuracy: 0.7590\n",
            "Epoch 6/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.5132 - accuracy: 0.8221 - val_loss: 0.7106 - val_accuracy: 0.7580\n",
            "Epoch 7/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.4095 - accuracy: 0.8568 - val_loss: 0.7623 - val_accuracy: 0.7560\n",
            "Epoch 8/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.3194 - accuracy: 0.8890 - val_loss: 0.7668 - val_accuracy: 0.7578\n",
            "Epoch 9/10\n",
            "1407/1407 [==============================] - 182s 130ms/step - loss: 0.2390 - accuracy: 0.9183 - val_loss: 0.8330 - val_accuracy: 0.7664\n",
            "Epoch 10/10\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.1758 - accuracy: 0.9403 - val_loss: 0.8937 - val_accuracy: 0.7618\n",
            "Best epoch: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(x_train, train_labels, epochs=best_epoch, validation_data=(val_images, val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wwCwifnwCG_",
        "outputId": "20e60e5e-321e-43e3-9245-578eb4acb808"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "1407/1407 [==============================] - 185s 131ms/step - loss: 1.5096 - accuracy: 0.4536 - val_loss: 1.2333 - val_accuracy: 0.5586\n",
            "Epoch 2/9\n",
            "1407/1407 [==============================] - 186s 132ms/step - loss: 1.0887 - accuracy: 0.6152 - val_loss: 0.9535 - val_accuracy: 0.6698\n",
            "Epoch 3/9\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.8791 - accuracy: 0.6939 - val_loss: 0.8651 - val_accuracy: 0.7018\n",
            "Epoch 4/9\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.7323 - accuracy: 0.7461 - val_loss: 0.7630 - val_accuracy: 0.7342\n",
            "Epoch 5/9\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.6174 - accuracy: 0.7866 - val_loss: 0.7130 - val_accuracy: 0.7576\n",
            "Epoch 6/9\n",
            "1407/1407 [==============================] - 182s 130ms/step - loss: 0.5138 - accuracy: 0.8228 - val_loss: 0.6827 - val_accuracy: 0.7662\n",
            "Epoch 7/9\n",
            "1407/1407 [==============================] - 182s 130ms/step - loss: 0.4080 - accuracy: 0.8590 - val_loss: 0.7038 - val_accuracy: 0.7664\n",
            "Epoch 8/9\n",
            "1407/1407 [==============================] - 182s 130ms/step - loss: 0.3187 - accuracy: 0.8898 - val_loss: 0.7659 - val_accuracy: 0.7582\n",
            "Epoch 9/9\n",
            "1407/1407 [==============================] - 183s 130ms/step - loss: 0.2416 - accuracy: 0.9167 - val_loss: 0.8096 - val_accuracy: 0.7626\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fda1dc6b050>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = hypermodel.evaluate(test_images, test_labels)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0jqmjQxwPFr",
        "outputId": "c395ae12-2470-43bf-8552-a67977dc9572"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 15s 47ms/step - loss: 0.8472 - accuracy: 0.7527\n",
            "[test loss, test accuracy]: [0.8472378849983215, 0.7526999711990356]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mo9rYGh3HOp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "umbvy1PFQ4Q2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}